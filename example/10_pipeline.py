from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval.metrics import GEval
from deepeval.dataset import EvaluationDataset
from deepeval import evaluate
from loguru import logger
import sys
import json

# Configure loguru for stylish output
logger.remove()
logger.add(
    sys.stderr, 
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>"
)
logger.add("pipeline.log", rotation="1 MB")

class QAEvaluationPipeline:
    """æœ¬ç•ªç’°å¢ƒã§ã®QAè©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    def __init__(self, judge_configs):
        self.judges = [self._create_judge(config) for config in judge_configs]
    def _create_judge(self, config):
        return GEval(
            name=config["name"],
            criteria=config["criteria"],
            evaluation_steps=config["steps"],
            evaluation_params=config["params"],
            threshold=config["threshold"],
            model=config.get("model", "gpt-4o-mini")
        )
    def evaluate_qa_batch(self, qa_pairs):
        """QAãƒšã‚¢ã®ãƒãƒƒãƒè©•ä¾¡"""
        logger.info(f"ğŸš€ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è©•ä¾¡ã‚’é–‹å§‹ - {len(qa_pairs)}ä»¶ã®QAãƒšã‚¢ã‚’å‡¦ç†")
        
        test_cases = []
        for i, qa in enumerate(qa_pairs, 1):
            logger.debug(f"ğŸ“„ {i}/{len(qa_pairs)}: ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’ä½œæˆä¸­...")
            test_cases.append(
                LLMTestCase(
                    input=qa["question"],
                    actual_output=qa["answer"],
                    expected_output=qa.get("expected", ""),
                    retrieval_context=qa.get("context", [])
                )
            )
        
        dataset = EvaluationDataset(test_cases=test_cases)
        logger.info(f"ğŸ¤– {len(self.judges)}å€‹ã®Judgeã§è©•ä¾¡å®Ÿè¡Œä¸­...")
        
        try:
            results = evaluate(test_cases=test_cases, metrics=self.judges)
            logger.success("âœ… ãƒãƒƒãƒè©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
            return self._format_results(results)
        except Exception as e:
            logger.error(f"âŒ ãƒãƒƒãƒè©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    def _format_results(self, results):
        """çµæœã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        logger.info("ğŸ“Š è©•ä¾¡çµæœã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¸­...")
        
        # Calculate overall metrics
        total_tests = len(results.test_results)
        passed_tests = sum(1 for result in results.test_results if result.success)
        success_rate = passed_tests / total_tests if total_tests > 0 else 0
        
        # Calculate average score across all metrics
        all_scores = []
        for result in results.test_results:
            for metric_data in result.metrics_data:
                if hasattr(metric_data, 'score') and metric_data.score is not None:
                    all_scores.append(metric_data.score)
        overall_score = sum(all_scores) / len(all_scores) if all_scores else 0
        
        formatted = {
            "overall_score": overall_score,
            "success_rate": success_rate,
            "detailed_results": []
        }
        
        logger.info(f"ğŸ¯ å…¨ä½“ã‚¹ã‚³ã‚¢: {overall_score:.3f}")
        logger.info(f"ğŸ† æˆåŠŸç‡: {success_rate:.1%}")
        
        for i, result in enumerate(results.test_results, 1):
            detailed = {
                "question": result.input,
                "answer": result.actual_output,
                "overall_success": result.success,
                "judge_scores": {}
            }
            
            logger.debug(f"ğŸ“ {i}: è©³ç´°çµæœã‚’å‡¦ç†ä¸­...")
            for metric_data in result.metrics_data:
                detailed["judge_scores"][metric_data.name] = {
                    "score": metric_data.score,
                    "success": metric_data.success,
                    "reason": getattr(metric_data, 'reason', '')
                }
            formatted["detailed_results"].append(detailed)
        
        return formatted

# ä½¿ç”¨ä¾‹
judge_configs = [
    {
        "name": "Accuracy",
        "criteria": "å›ç­”ã®äº‹å®Ÿçš„æ­£ç¢ºæ€§ã‚’è©•ä¾¡",
        "steps": ["äº‹å®Ÿç¢ºèª", "æ­£ç¢ºæ€§åˆ¤å®š"],
        "params": [LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],
        "threshold": 0.8
    },
    {
        "name": "Helpfulness", 
        "criteria": "å›ç­”ã®æœ‰ç”¨æ€§ã‚’è©•ä¾¡",
        "steps": ["æœ‰ç”¨æ€§ç¢ºèª", "å®Ÿç”¨æ€§åˆ¤å®š"],
        "params": [LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],
        "threshold": 0.7
    }
]

pipeline = QAEvaluationPipeline(judge_configs)

# è©•ä¾¡å®Ÿè¡Œ
qa_data = [
    {
        "question": "Python ã®ç‰¹å¾´ã¯ï¼Ÿ",
        "answer": "Pythonã¯èª­ã¿ã‚„ã™ãã€å­¦ç¿’ã—ã‚„ã™ã„ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã§ã™ã€‚",
        "expected": "Pythonã¯é«˜æ°´æº–ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã§ã€ã‚·ãƒ³ãƒ—ãƒ«ãªæ–‡æ³•ãŒç‰¹å¾´ã§ã™ã€‚"
    }
]

try:
    evaluation_results = pipeline.evaluate_qa_batch(qa_data)
    
    logger.success(f"ğŸ‰ è©•ä¾¡å®Œäº† - å…¨ä½“ã‚¹ã‚³ã‚¢: {evaluation_results['overall_score']:.3f}")
    logger.info(f"ğŸ“Š æˆåŠŸç‡: {evaluation_results['success_rate']:.1%}")
    
    # çµæœã‚’JSONå½¢å¼ã§è¡¨ç¤º
    logger.info("ğŸ“‹ è©³ç´°çµæœ:")
    print(json.dumps(evaluation_results, ensure_ascii=False, indent=2))
    
except Exception as e:
    logger.error(f"âŒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
    raise