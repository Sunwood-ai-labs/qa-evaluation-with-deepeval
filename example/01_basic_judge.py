from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval.metrics import GEval
from loguru import logger
import sys
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
load_dotenv()

# LLM as a Judge ãƒ¡ãƒˆãƒªãƒƒã‚¯å®šç¾©
correctness_judge = GEval(
    name="Correctness",
    criteria="""
    å›ç­”ãŒè³ªå•ã«å¯¾ã—ã¦äº‹å®Ÿçš„ã«æ­£ç¢ºã§å®Œå…¨ã‹ã©ã†ã‹ã‚’è©•ä¾¡ã™ã‚‹
    
    è©•ä¾¡ã¯æ—¥æœ¬èªã§ç†ç”±ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
    """,
    evaluation_steps=[
        "è³ªå•ã®è¦æ±‚å†…å®¹ã‚’ç†è§£ã™ã‚‹",
        "å®Ÿéš›ã®å›ç­”ã®å†…å®¹ã‚’åˆ†æã™ã‚‹",
        "æœŸå¾…ã•ã‚Œã‚‹å›ç­”ã¨æ¯”è¼ƒã™ã‚‹",
        "äº‹å®Ÿã®æ­£ç¢ºæ€§ã‚’ç¢ºèªã™ã‚‹",
        "0-5ã®ã‚¹ã‚³ã‚¢ã§è©•ä¾¡ã™ã‚‹"
    ],
    evaluation_params=[
        LLMTestCaseParams.INPUT,
        LLMTestCaseParams.ACTUAL_OUTPUT,
        LLMTestCaseParams.EXPECTED_OUTPUT
    ],
    threshold=0.7,
    model="gpt-4o-mini"
)

# ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
test_case = LLMTestCase(
    input="æ—¥æœ¬ã®é¦–éƒ½ã¯ã©ã“ã§ã™ã‹ï¼Ÿ",
    actual_output="æ—¥æœ¬ã®é¦–éƒ½ã¯æ±äº¬ã§ã™ã€‚",
    expected_output="æ—¥æœ¬ã®é¦–éƒ½ã¯æ±äº¬éƒ½ã§ã™ã€‚äººå£ã¯ç´„1400ä¸‡äººã§ã€æ”¿æ²»ãƒ»çµŒæ¸ˆã®ä¸­å¿ƒåœ°ã§ã™ã€‚"
)

# è©•ä¾¡å®Ÿè¡Œ
logger.info("ğŸ¤– LLM Judgeè©•ä¾¡ã‚’é–‹å§‹ã—ã¾ã™")
try:
    correctness_judge.measure(test_case)
    logger.success(f"âœ… è©•ä¾¡å®Œäº† - ã‚¹ã‚³ã‚¢: {correctness_judge.score}")
    logger.info(f"ğŸ’­ è©•ä¾¡ç†ç”±: {correctness_judge.reason}")
except Exception as e:
    logger.error(f"âŒ è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    raise