import pandas as pd
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval.metrics import GEval
from loguru import logger
import sys
import os
from dotenv import load_dotenv
from litellm_model import LiteLLMModel
from langfuse import Langfuse

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
load_dotenv()

# loguruã®æ—¥æœ¬èªã‚«ã‚¹ã‚¿ãƒ å‡ºåŠ›è¨­å®š
logger.remove()
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
    colorize=True
)
logger.add("litellm_japanese_evaluation_batch.log", rotation="1 MB", encoding="utf-8")

# Langfuseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
langfuse = Langfuse(
    secret_key=os.environ.get("LANGFUSE_SECRET_KEY", ""),
    public_key=os.environ.get("LANGFUSE_PUBLIC_KEY", ""),
    host=os.environ.get("LANGFUSE_HOST", "https://cloud.langfuse.com")
)

def create_test_cases_from_csv(csv_path):
    df = pd.read_csv(csv_path)
    test_cases = []
    for _, row in df.iterrows():
        test_case = LLMTestCase(
            input=row["question"],
            actual_output=row["llm_answer"],
            expected_output=row["expected_answer"],
            retrieval_context=row.get("context", "").split("|||") if pd.notna(row.get("context")) else []
        )
        test_cases.append(test_case)
    return test_cases

def main():
    # LiteLLMãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
    model_name = os.environ.get("LITELLM_MODEL", "gpt-4o-mini")
    api_base = os.environ.get("LITELLM_BASE_URL", "http://localhost:4000")
    api_key = os.environ.get("LITELLM_API_KEY", "your-api-key")

    # LiteLLMãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
    custom_model = LiteLLMModel(
        model_name=model_name,
        base_url=api_base,
        api_key=api_key
    )

    # æ—¥æœ¬èªå°‚ç”¨GEvalãƒ¡ãƒˆãƒªãƒƒã‚¯
    correctness_judge = GEval(
        name="Correctness",
        criteria="""
ã‚ãªãŸã¯å„ªç§€ãªæ—¥æœ¬èªè©•ä¾¡è€…ã§ã™ã€‚ä»¥ä¸‹ã®åŸºæº–ã§è©•ä¾¡ã—ã¦ãã ã•ã„ï¼š

å›ç­”ãŒè³ªå•ã«å¯¾ã—ã¦äº‹å®Ÿçš„ã«æ­£ç¢ºã§å®Œå…¨ã‹ã©ã†ã‹ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

é‡è¦ãªæŒ‡ç¤ºï¼š
- è©•ä¾¡ç†ç”±ã¯å¿…ãšæ—¥æœ¬èªã§è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„
- æ—¥æœ¬èªä»¥å¤–ã§ã®èª¬æ˜ã¯çµ¶å¯¾ã«ç¦æ­¢ã§ã™
- ä»¥ä¸‹ã®è¦³ç‚¹ã‹ã‚‰è©•ä¾¡ã—ã¦ãã ã•ã„ï¼š
  * äº‹å®Ÿã®æ­£ç¢ºæ€§
  * å›ç­”ã®å®Œå…¨æ€§  
  * æœŸå¾…ã•ã‚Œã‚‹å›ç­”ã¨ã®ä¸€è‡´åº¦

æœ€çµ‚çš„ãªè©•ä¾¡ç†ç”±ã¯æ—¥æœ¬èªã§æ›¸ã„ã¦ãã ã•ã„ã€‚
""",
        evaluation_steps=[
            "è³ªå•ã®è¦æ±‚å†…å®¹ã‚’æ—¥æœ¬èªã§ç†è§£ã—ã€ä½•ãŒæ±‚ã‚ã‚‰ã‚Œã¦ã„ã‚‹ã‹ã‚’æ˜ç¢ºã«ã™ã‚‹",
            "å®Ÿéš›ã®å›ç­”ã®å†…å®¹ã‚’æ—¥æœ¬èªã§è©³ã—ãåˆ†æã—ã€å«ã¾ã‚Œã¦ã„ã‚‹æƒ…å ±ã‚’æ•´ç†ã™ã‚‹",
            "æœŸå¾…ã•ã‚Œã‚‹å›ç­”ã¨å®Ÿéš›ã®å›ç­”ã‚’æ—¥æœ¬èªã§æ¯”è¼ƒã—ã€å·®ç•°ã‚’ç‰¹å®šã™ã‚‹",
            "äº‹å®Ÿã®æ­£ç¢ºæ€§ã‚’æ—¥æœ¬èªã§ç¢ºèªã—ã€é–“é•ã„ã‚„ä¸è¶³ãŒãªã„ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹",
            "0-10ã®ã‚¹ã‚³ã‚¢ã§è©•ä¾¡ã—ã€ãã®ç†ç”±ã‚’å¿…ãšæ—¥æœ¬èªã§è©³ã—ãèª¬æ˜ã™ã‚‹",
            "è©•ä¾¡ã®æœ€çµ‚çµæœã‚’æ—¥æœ¬èªã§ã¾ã¨ã‚ã€æ”¹å–„ç‚¹ãŒã‚ã‚Œã°æ—¥æœ¬èªã§ææ¡ˆã™ã‚‹"
        ],
        evaluation_params=[
            LLMTestCaseParams.INPUT,
            LLMTestCaseParams.ACTUAL_OUTPUT,
            LLMTestCaseParams.EXPECTED_OUTPUT
        ],
        threshold=0.3,
        model=custom_model
    )

    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’CSVã‹ã‚‰ç”Ÿæˆ
    test_cases = create_test_cases_from_csv("example/qa_dataset.csv")

    logger.info(f"ğŸ“ ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹æ•°: {len(test_cases)} ä»¶")
    success_count = 0
    fail_count = 0

    for idx, test_case in enumerate(test_cases, 1):
        logger.info(f"==== {idx}ä»¶ç›®ã®è©•ä¾¡ã‚’é–‹å§‹ ====")
        try:
            logger.debug(f"   å…¥åŠ›: {test_case.input}")
            logger.debug(f"   å®Ÿéš›ã®å‡ºåŠ›: {test_case.actual_output}")
            logger.debug(f"   æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›: {test_case.expected_output}")

            logger.info("âš¡ è©•ä¾¡å‡¦ç†ä¸­...")
            correctness_judge.measure(test_case)

            score = correctness_judge.score
            threshold = correctness_judge.threshold
            passed = score >= threshold
            reason = correctness_judge.reason

            logger.info("ğŸ“Š è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼:")
            logger.info(f"   ğŸ¯ ã‚¹ã‚³ã‚¢: {score:.3f}")
            logger.info(f"   ğŸšï¸  ã—ãã„å€¤: {threshold}")
            logger.info(f"   ğŸ† åˆ¤å®š: {'âœ… åˆæ ¼' if passed else 'âŒ ä¸åˆæ ¼'}")

            logger.info("ğŸ“ è©•ä¾¡ç†ç”±:")
            for line in reason.split('\n'):
                if line.strip():
                    logger.info(f"   {line.strip()}")

            # Langfuseã«è©•ä¾¡çµæœã‚’é€ä¿¡
            logger.info("ğŸŒ Langfuseã«è©•ä¾¡çµæœã‚’é€ä¿¡ã—ã¾ã™")
            with langfuse.start_as_current_span(name="LiteLLMæ—¥æœ¬èªJudgeè©•ä¾¡ Case15") as span:
                span.update_trace(
                    input={
                        "input": test_case.input,
                        "actual_output": test_case.actual_output,
                        "expected_output": test_case.expected_output
                    },
                    output={
                        "score": score,
                        "threshold": threshold,
                        "passed": passed,
                        "reason": reason
                    },
                    metadata={
                        "model": model_name,
                        "evaluation_type": "correctness",
                        "language": "japanese"
                    }
                )
                langfuse.score_current_trace(
                    name="correctness_score",
                    value=score,
                    data_type="NUMERIC",
                    comment=f"è©•ä¾¡ç†ç”±: {reason[:100]}..."
                )
            logger.success("ğŸš€ Langfuseã¸ã®é€ä¿¡ãŒå®Œäº†ã—ã¾ã—ãŸ")

            if passed:
                logger.success(f"ğŸ‰ ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ãŒåˆæ ¼ã—ã¾ã—ãŸ (ã‚¹ã‚³ã‚¢: {score:.3f} >= ã—ãã„å€¤: {threshold})")
                success_count += 1
            else:
                logger.warning(f"âš ï¸  ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ãŒä¸åˆæ ¼ã§ã™ (ã‚¹ã‚³ã‚¢: {score:.3f} < ã—ãã„å€¤: {threshold})")
                fail_count += 1

        except Exception as e:
            logger.error(f"âŒ è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
            logger.error(f"   ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}")
            logger.error(f"   ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}")
            fail_count += 1
        finally:
            try:
                langfuse.flush()
                logger.info("ğŸ§¹ Langfuseãƒ‡ãƒ¼ã‚¿ã®é€ä¿¡å®Œäº†ã‚’ç¢ºèªã—ã¾ã—ãŸ")
            except Exception as e:
                logger.warning(f"âš ï¸ Langfuseã®flushä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")

    logger.info("ğŸ ãƒãƒƒãƒè©•ä¾¡å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
    logger.info(f"âœ… åˆæ ¼: {success_count}ä»¶, âŒ ä¸åˆæ ¼: {fail_count}ä»¶, åˆè¨ˆ: {len(test_cases)}ä»¶")

if __name__ == "__main__":
    main()