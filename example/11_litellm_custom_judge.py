from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval.metrics import GEval
from deepeval.models.base_model import DeepEvalBaseLLM
from loguru import logger
import sys
import os
import requests
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
load_dotenv()

# LiteLLMã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«
class LiteLLMModel(DeepEvalBaseLLM):
    def __init__(self, model_name, base_url, api_key):
        self.model_name = model_name
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key
    
    def load_model(self):
        # LiteLLMã¯å¤–éƒ¨APIãªã®ã§ã€ã“ã“ã§ã¯è¨­å®šã‚’è¿”ã™ã ã‘
        return {
            "model": self.model_name,
            "base_url": self.base_url,
            "api_key": self.api_key
        }
    
    def generate(self, prompt: str) -> str:
        config = self.load_model()
        
        # OpenAIäº’æ›APIã§LiteLLMã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        url = f"{config['base_url']}/chat/completions"
        headers = {
            "Authorization": f"Bearer {config['api_key']}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": config["model"],
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.1,
            "max_tokens": 2000
        }
        
        try:
            response = requests.post(url, headers=headers, json=data)
            response.raise_for_status()
            result = response.json()
            return result["choices"][0]["message"]["content"]
        except Exception as e:
            logger.error(f"LiteLLM API error: {e}")
            raise
    
    async def a_generate(self, prompt: str) -> str:
        # åŒæœŸç‰ˆã‚’å†åˆ©ç”¨ï¼ˆç°¡å˜ãªå®Ÿè£…ï¼‰
        return self.generate(prompt)
    
    def get_model_name(self):
        return f"LiteLLM-{self.model_name}"

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—
model_name = os.environ.get("LITELLM_MODEL", "gpt-4o-mini")
api_base = os.environ.get("LITELLM_BASE_URL", "http://localhost:4000")
api_key = os.environ.get("LITELLM_API_KEY", "your-api-key")

# ã‚«ã‚¹ã‚¿ãƒ LiteLLMãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
custom_model = LiteLLMModel(
    model_name=model_name,
    base_url=api_base,
    api_key=api_key
)

# LLM as a Judge ãƒ¡ãƒˆãƒªãƒƒã‚¯å®šç¾©
correctness_judge = GEval(
    name="Correctness",
    criteria="""
    å›ç­”ãŒè³ªå•ã«å¯¾ã—ã¦äº‹å®Ÿçš„ã«æ­£ç¢ºã§å®Œå…¨ã‹ã©ã†ã‹ã‚’è©•ä¾¡ã™ã‚‹

    è©•ä¾¡ã¯æ—¥æœ¬èªã§ç†ç”±ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
    """,
    evaluation_steps=[
        "è³ªå•ã®è¦æ±‚å†…å®¹ã‚’ç†è§£ã™ã‚‹",
        "å®Ÿéš›ã®å›ç­”ã®å†…å®¹ã‚’åˆ†æã™ã‚‹",
        "æœŸå¾…ã•ã‚Œã‚‹å›ç­”ã¨æ¯”è¼ƒã™ã‚‹",
        "äº‹å®Ÿã®æ­£ç¢ºæ€§ã‚’ç¢ºèªã™ã‚‹",
        "0-5ã®ã‚¹ã‚³ã‚¢ã§è©•ä¾¡ã™ã‚‹"
    ],
    evaluation_params=[
        LLMTestCaseParams.INPUT,
        LLMTestCaseParams.ACTUAL_OUTPUT,
        LLMTestCaseParams.EXPECTED_OUTPUT
    ],
    threshold=0.7,
    model=custom_model  # ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½¿ç”¨
)

# ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
test_case = LLMTestCase(
    input="æ—¥æœ¬ã®é¦–éƒ½ã¯ã©ã“ã§ã™ã‹ï¼Ÿ",
    actual_output="æ—¥æœ¬ã®é¦–éƒ½ã¯æ±äº¬ã§ã™ã€‚",
    expected_output="æ—¥æœ¬ã®é¦–éƒ½ã¯æ±äº¬éƒ½ã§ã™ã€‚äººå£ã¯ç´„1400ä¸‡äººã§ã€æ”¿æ²»ãƒ»çµŒæ¸ˆã®ä¸­å¿ƒåœ°ã§ã™ã€‚"
)

# è©•ä¾¡å®Ÿè¡Œ
logger.info("ğŸ¤– LiteLLM Judgeè©•ä¾¡ã‚’é–‹å§‹ã—ã¾ã™")
try:
    correctness_judge.measure(test_case)
    logger.success(f"âœ… è©•ä¾¡å®Œäº† - ã‚¹ã‚³ã‚¢: {correctness_judge.score}")
    logger.info(f"ğŸ’­ è©•ä¾¡ç†ç”±: {correctness_judge.reason}")
except Exception as e:
    logger.error(f"âŒ è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    raise